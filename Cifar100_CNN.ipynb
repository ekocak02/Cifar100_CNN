{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f546f494-178f-4864-8fa6-66ad4e9040e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a CNN model with cifar100 dataset\n",
    "#Import Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61736d21-d9b8-4f7c-b99a-0d88cc7bed6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data and normalize image data\n",
    "(X_train, y_train), (X_test, y_test) = cifar100.load_data()\n",
    "\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46823327-ee9e-486f-bd99-72332ae43b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of all classes in CIFAR-100\n",
    "classes = ['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', \n",
    "           'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', \n",
    "           'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'computer_keyboard', \n",
    "           'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', \n",
    "           'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', \n",
    "           'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', \n",
    "           'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', \n",
    "           'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', \n",
    "           'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c57c9da-715c-4c30-bd5a-ab3aa699b26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization\n",
    "plt.figure(figsize=(20, 3))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(X_test[i], cmap=\"gray\")\n",
    "    plt.xlabel(classes[y_test[i].astype(int)[0]])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aaef2f-2878-49cf-a696-686a4be10081",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding label data\n",
    "'''\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "y_train=enc.fit_transform(y_train).toarray().astype(int)\n",
    "y_test=enc.transform(y_test).toarray().astype(int)\n",
    "\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_train[0])\n",
    "'''\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(y_train, 100)\n",
    "y_test = to_categorical(y_test, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f76748-535f-461a-813b-b5483b87e0d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#the process of enlarging images from 32x32 to 64x64 size\n",
    "\n",
    "#X_train = np.array([tf.image.resize(img, (64,64)) for img in X_train])\n",
    "#X_test = np.array([tf.image.resize(img, (64,64)) for img in X_test])\n",
    "\n",
    "#Smart_resize is selected due to the amount of RAM.\n",
    "X_train = np.array([tf.keras.preprocessing.image.smart_resize(img, (64,64)) for img in X_train])\n",
    "X_test = np.array([tf.keras.preprocessing.image.smart_resize(img, (64,64)) for img in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4a7f94-b759-4aad-a1ed-de98fb587f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the process of creating validation data (10% of the test data was used)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_train, y_train, test_size=0.5, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21112c9-01c8-441a-a69a-7cf885605398",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate augmented images\n",
    "datagen = ImageDataGenerator(\n",
    "            rotation_range = 20, #Rotation up to 20 degrees.\n",
    "            width_shift_range = 0.2, #20% horizontal scrolling.\n",
    "            height_shift_range = 0.2, #20% vertical scrolling.\n",
    "            shear_range = 0.2, #20% scrolling on the image.\n",
    "            zoom_range = 0.2, #20% zoom in-out on the image.\n",
    "            horizontal_flip = True, #inverting an image horizontally.\n",
    "            fill_mode = 'nearest' #The process of filling empty areas with the closest pixels.\n",
    ")\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea08e49-603a-4a96-a24c-79c29174e5c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Build CNN model\n",
    "\n",
    "#First feature extraction layer\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32,(3,3), padding='same', input_shape = (64,64,3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Conv2D(32,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "#Second feature extraction layer\n",
    "model.add(Conv2D(32,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Conv2D(64,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Conv2D(64,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "#Third feature extraction layer\n",
    "model.add(Conv2D(64,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Conv2D(128,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Conv2D(128,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "#Fourth feature extraction layer\n",
    "model.add(Conv2D(256,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Conv2D(256,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Conv2D(256,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "#Classification layer\n",
    "model.add(Dense(1024))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization(momentum = 0.95))\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e48ce29-c04c-43ab-8460-446c6c01e046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Compile\n",
    "model.compile(optimizer=Adam(decay = 1e-6),\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042dee80-4faa-4958-a3be-3cf41d284de9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Model fit with early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=7, restore_best_weights=True)\n",
    "history = model.fit(datagen.flow(X_train, y_train, batch_size = 256),\n",
    "                    validation_data = (X_val, y_val),\n",
    "                    epochs = 100,\n",
    "                    callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38427a52-e008-4a99-a897-8705416a0050",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating the model with test data\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05323b54-2389-4e1a-9e02-62b250015f72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#the process of plotting accuracy and loss using a line graph\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['accuracy'], marker = 'o', label = 'Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], marker = 'x', label = 'Validation Accuracy')\n",
    "plt.title('Acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'], marker = 'o', label = 'Training Loss')\n",
    "plt.plot(history.history['val_loss'], marker = 'x', label = 'Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9eaff4-df21-4f4f-b81b-c2a1282d9620",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model save\n",
    "model.save('cifar100_CNN.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353a724b-1de6-4725-8707-a8490660f30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('cifar100_CNN.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510fe4bd-5999-4f08-85a8-687ad0c67660",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Prediction with test dataset\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f54d8e-19f2-4263-9d8a-1077d1f55a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "prediction = []\n",
    "true_labels = []\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "  prediction.append(np.argmax(y_pred[i]))\n",
    "  true_labels.append(np.argmax(y_test[i]))\n",
    "\n",
    "cm = confusion_matrix(prediction, true_labels)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56d1ca4-e0ae-48ab-b5e4-2f086ad5a102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating f1 score\n",
    "from sklearn.metrics import f1_score\n",
    "print(f\"f1 score: {f1_score(true_labels, prediction, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec832c6-ef52-4f0b-ac76-a9d7ea2ed87e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(true_labels, prediction, target_names=classes, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64a3699-16b2-449b-8199-f0c2a2b93c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(24,24))\n",
    "ax = fig.add_subplot(211)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + classes)\n",
    "ax.set_yticklabels([''] + classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d485561-3b22-4a70-befb-23df9a911e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
